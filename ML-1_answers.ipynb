{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7cdf3d2",
   "metadata": {},
   "source": [
    "# Answers to Assignment Questions (Feature Engineering / ML)\n",
    "\n",
    "This notebook contains concise answers to the questions from the provided PDF. You can open and run this notebook in Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff725f85",
   "metadata": {},
   "source": [
    "## What is a parameter?\n",
    "\n",
    "A **parameter** is a configuration variable of a model that is learned from training data. For example, in linear regression y = wx + b, the parameters are **w** (weight/slope) and **b** (bias/intercept). Model training adjusts parameters to minimize a loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f588e",
   "metadata": {},
   "source": [
    "## What is correlation?\n",
    "\n",
    "**Correlation** measures the strength and direction of a linear relationship between two variables. It is commonly quantified by the Pearson correlation coefficient (r), which ranges from -1 (perfect negative linear relationship) to +1 (perfect positive linear relationship). A value of 0 indicates no linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fbaf99",
   "metadata": {},
   "source": [
    "## What does negative correlation mean?\n",
    "\n",
    "A **negative correlation** means that as one variable increases, the other tends to decrease. For example, if correlation r = -0.8 between hours spent watching TV and test score, higher TV time is associated with lower test scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e18e695",
   "metadata": {},
   "source": [
    "## Define Machine Learning. What are the main components in Machine Learning?\n",
    "\n",
    "**Machine Learning (ML)** is a field of computer science that builds algorithms that learn patterns from data to make predictions or decisions without being explicitly programmed for every case. Main components:\n",
    "\n",
    "- **Data**: labeled or unlabeled examples used to train and evaluate models.\n",
    "- **Features**: input variables derived from raw data.\n",
    "- **Model/Algorithm**: the mathematical structure (e.g., linear regression, decision tree, neural network).\n",
    "- **Loss/Objective function**: measures how well the model performs.\n",
    "- **Optimizer**: method to update parameters to minimize loss (e.g., gradient descent).\n",
    "- **Evaluation metrics**: accuracy, precision, recall, RMSE, etc.\n",
    "- **Training/Validation/Test split**: to train and evaluate generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbe54cc",
   "metadata": {},
   "source": [
    "## How does loss value help in determining whether the model is good or not?\n",
    "\n",
    "The **loss value** quantifies how far model predictions are from true targets on the dataset used. Lower loss generally indicates better fit. However, absolute loss values depend on the task and loss type (e.g., MSE vs cross-entropy). You must monitor validation loss (not just training loss) to detect overfitting. Also combine loss with task-specific metrics (accuracy, F1, RMSE) for a fuller picture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75196d28",
   "metadata": {},
   "source": [
    "## What are continuous and categorical variables?\n",
    "\n",
    "- **Continuous variables** (numerical) can take on any real value within a range (e.g., height, weight, price).\n",
    "- **Categorical variables** represent discrete groups or categories (e.g., color: red/green/blue; country names). Categorical variables can be nominal (no order) or ordinal (ordered)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b155ff82",
   "metadata": {},
   "source": [
    "## How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    "Common techniques:\n",
    "\n",
    "- **Label Encoding**: map categories to integers (useful for ordinal categories or tree-based models but can introduce artificial order).\n",
    "- **One-Hot Encoding**: create binary columns for each category (works well with linear models and neural nets; increases dimensionality).\n",
    "- **Target/Mean Encoding**: replace category with target mean (requires careful cross-validation to avoid leakage).\n",
    "- **Binary/Hash Encoding**: useful when there are many categories (reduces dimensionality).\n",
    "- **Embedding layers**: learned dense representations for categories (common in deep learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fac3d9",
   "metadata": {},
   "source": [
    "## What do you mean by training and testing a dataset?\n",
    "\n",
    "- **Training**: using a dataset to update model parameters (learning). The model sees inputs and targets and optimizes its parameters to minimize loss.\n",
    "- **Testing**: evaluating the trained model on unseen data (the test set) to estimate how well it generalizes to new inputs. Test data must not influence training or hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bece71",
   "metadata": {},
   "source": [
    "## What is sklearn.preprocessing?\n",
    "\n",
    "`sklearn.preprocessing` is a module in scikit-learn that provides utilities to transform input data—scaling, encoding, normalization, handling missing values, and generating polynomial features. Examples include `StandardScaler`, `MinMaxScaler`, `LabelEncoder`, `OneHotEncoder`, and `PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2913f505",
   "metadata": {},
   "source": [
    "## What is a Test set?\n",
    "\n",
    "The **test set** is a portion of the dataset held out from training and hyperparameter tuning, used only once (or occasionally) to estimate final model performance. It simulates performance on truly unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec97ab0a",
   "metadata": {},
   "source": [
    "## How do we split data for model fitting (training and testing) in Python?\n",
    "\n",
    "Commonly using `train_test_split` from scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "You can also create a validation set or use cross-validation (`KFold`, `StratifiedKFold`) for robust hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b4ac7",
   "metadata": {},
   "source": [
    "## How do you approach a Machine Learning problem?\n",
    "\n",
    "A typical approach:\n",
    "\n",
    "1. **Define objective**: what to predict and how success is measured.\n",
    "2. **Collect/understand data**: gather required datasets.\n",
    "3. **Exploratory Data Analysis (EDA)**: inspect distributions, missing values, correlations, outliers.\n",
    "4. **Preprocess/clean**: handle missing values, encode categories, scale features.\n",
    "5. **Feature engineering**: create or select informative features.\n",
    "6. **Model selection & training**: choose algorithms, train models.\n",
    "7. **Validate & tune**: cross-validation and hyperparameter tuning.\n",
    "8. **Evaluate**: test on hold-out set and compute metrics.\n",
    "9. **Deploy & monitor**: serve the model and track performance in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528a0c4a",
   "metadata": {},
   "source": [
    "## Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    "**EDA (Exploratory Data Analysis)** helps to:\n",
    "- Discover patterns, trends, and relationships\n",
    "- Detect missing values and outliers\n",
    "- Identify wrong data types or inconsistent entries\n",
    "- Inform feature engineering and model selection\n",
    "- Avoid garbage-in–garbage-out by improving data quality before modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55186da9",
   "metadata": {},
   "source": [
    "## How can you find correlation between variables in Python?\n",
    "\n",
    "Using Pandas and visualization:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "corr_matrix = df.corr()           # Pearson correlation for numeric columns\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "```\n",
    "For pairwise visual checks use `sns.pairplot(df)` or `df.corr(method='spearman')` for monotonic relationships. For categorical variables, use `Cramér's V` or contingency tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdff6a2",
   "metadata": {},
   "source": [
    "## What is causation? Explain difference between correlation and causation with an example.\n",
    "\n",
    "**Causation** means one variable directly affects another. **Correlation** means two variables move together but one may not cause the other.\n",
    "\n",
    "Example: Ice cream sales and drowning rates are positively correlated (both rise in summer), but buying ice cream doesn't cause drowning—**temperature** is a confounder that causes both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62251bcd",
   "metadata": {},
   "source": [
    "## What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    "An **optimizer** is an algorithm that updates model parameters to minimize the loss function. Types:\n",
    "\n",
    "- **Gradient Descent (Batch GD)**: uses the full dataset to compute gradients and update params. Good for small datasets.\n",
    "- **Stochastic Gradient Descent (SGD)**: updates parameters using one sample at a time—faster but noisier.\n",
    "- **Mini-batch Gradient Descent**: uses small batches (common in deep learning).\n",
    "- **Momentum**: accelerates SGD by adding a fraction of the previous update to the current one.\n",
    "- **AdaGrad**: adapts learning rates per parameter based on past squared gradients—good for sparse data.\n",
    "- **RMSprop**: fixes AdaGrad's learning-rate decay by using moving averages of squared gradients.\n",
    "- **Adam**: combines momentum and RMSprop ideas; widely used in training neural networks.\n",
    "\n",
    "Example: `optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)` in TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aebb98",
   "metadata": {},
   "source": [
    "## What is sklearn.linear_model ?\n",
    "\n",
    "`sklearn.linear_model` is a scikit-learn module containing linear models like `LinearRegression`, `LogisticRegression`, `Ridge`, `Lasso`, and `ElasticNet`. These implement simple and regularized linear models for regression and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb19ba46",
   "metadata": {},
   "source": [
    "## What does model.fit() do? What arguments must be given?\n",
    "\n",
    "`model.fit()` trains the model by adjusting its parameters to the training data.\n",
    "- Typical arguments: `X_train` (features) and `y_train` (targets). Some models accept `sample_weight`, `epochs` (in Keras), or `validation_data` depending on the API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b507de9c",
   "metadata": {},
   "source": [
    "## What does model.predict() do? What arguments must be given?\n",
    "\n",
    "`model.predict()` returns predictions from the trained model for new input data. The main argument is `X` (feature matrix). For classifiers, scikit-learn also has `predict_proba()` to get class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e1ff0f",
   "metadata": {},
   "source": [
    "## What is feature scaling? How does it help in Machine Learning? How do we perform scaling in Python?\n",
    "\n",
    "**Feature scaling** is transforming numeric features to a similar scale so that some algorithms (gradient descent-based, distance-based models like KNN, SVM) converge faster or behave correctly.\n",
    "\n",
    "Common methods:\n",
    "- **Standardization (Z-score)**: subtract mean and divide by std (`StandardScaler`).\n",
    "- **Min-Max Scaling**: scale values to [0,1] range (`MinMaxScaler`).\n",
    "- **Robust Scaling**: uses median and IQR to reduce effect of outliers (`RobustScaler`).\n",
    "\n",
    "In Python (scikit-learn):\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc33d27",
   "metadata": {},
   "source": [
    "## Explain data encoding?\n",
    "\n",
    "**Data encoding** transforms categorical or non-numeric data into numeric form suitable for ML models. Techniques include label encoding, one-hot encoding, ordinal encoding, target encoding, and embeddings. For textual data, encoding may also include TF-IDF or word embeddings (Word2Vec, BERT)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c8d8a4",
   "metadata": {},
   "source": [
    "## Notes / Final tips\n",
    "\n",
    "- Always avoid data leakage (information from test/validation leaking into training).\n",
    "- Use cross-validation for robust evaluation.\n",
    "- Document preprocessing steps and save any fitted transformers (scalers, encoders) to apply consistently to new data.\n",
    "- For reproducibility, set `random_state`/`seed` when splitting data or training models."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
